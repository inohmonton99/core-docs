(window.webpackJsonp=window.webpackJsonp||[]).push([[30],{122:function(e,n,t){"use strict";t.d(n,"a",(function(){return l})),t.d(n,"b",(function(){return d}));var a=t(0),s=t.n(a);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function c(e,n){if(null==e)return{};var t,a,s=function(e,n){if(null==e)return{};var t,a,s={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(s[t]=e[t]);return s}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(s[t]=e[t])}return s}var m=s.a.createContext({}),p=function(e){var n=s.a.useContext(m),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},l=function(e){var n=p(e.components);return s.a.createElement(m.Provider,{value:n},e.children)},u={inlineCode:"code",wrapper:function(e){var n=e.children;return s.a.createElement(s.a.Fragment,{},n)}},f=s.a.forwardRef((function(e,n){var t=e.components,a=e.mdxType,r=e.originalType,i=e.parentName,m=c(e,["components","mdxType","originalType","parentName"]),l=p(t),f=a,d=l["".concat(i,".").concat(f)]||l[f]||u[f]||r;return t?s.a.createElement(d,o(o({ref:n},m),{},{components:t})):s.a.createElement(d,o({ref:n},m))}));function d(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var r=t.length,i=new Array(r);i[0]=f;var o={};for(var c in n)hasOwnProperty.call(n,c)&&(o[c]=n[c]);o.originalType=e,o.mdxType="string"==typeof e?e:a,i[1]=o;for(var m=2;m<r;m++)i[m]=t[m];return s.a.createElement.apply(null,i)}return s.a.createElement.apply(null,t)}f.displayName="MDXCreateElement"},233:function(e,n,t){"use strict";t.r(n),n.default=t.p+"assets/images/metrics-230759-a20681c8f267210e43423be97e890665.png"},90:function(e,n,t){"use strict";t.r(n),t.d(n,"frontMatter",(function(){return r})),t.d(n,"metadata",(function(){return i})),t.d(n,"rightToc",(function(){return o})),t.d(n,"default",(function(){return m}));var a=t(2),s=(t(0),t(122));const r={title:"Persisting training metrics",sidebar_label:"Persisting training metrics",description:"Onepanel - Persisting training metrics"},i={unversionedId:"reference/workflows/metrics",id:"reference/workflows/metrics",isDocsHomePage:!1,title:"Persisting training metrics",description:"Onepanel - Persisting training metrics",source:"@site/docs/reference/workflows/metrics.md",slug:"/reference/workflows/metrics",permalink:"/docs/reference/workflows/metrics",editUrl:"https://github.com/onepanelio/core-docs/tree/master/docs/reference/workflows/metrics.md",version:"current",sidebar_label:"Persisting training metrics",sidebar:"reference",previous:{title:"Workflow Templates",permalink:"/docs/reference/workflows/templates"},next:{title:"Creating a Workflow Template",permalink:"/docs/reference/workflows/create"}},o=[{value:"Persisting metrics metrics in tasks",id:"persisting-metrics-metrics-in-tasks",children:[]},{value:"Passing metrics between tasks",id:"passing-metrics-between-tasks",children:[]},{value:"Persisting metrics in Workflows",id:"persisting-metrics-in-workflows",children:[]},{value:"Advance example",id:"advance-example",children:[]}],c={rightToc:o};function m({components:e,...n}){return Object(s.b)("wrapper",Object(a.a)({},c,n,{components:e,mdxType:"MDXLayout"}),Object(s.b)("h2",{id:"persisting-metrics-metrics-in-tasks"},"Persisting metrics metrics in tasks"),Object(s.b)("p",null,"You can persist metrics into as JSON in a special file ",Object(s.b)("inlineCode",{parentName:"p"},"/tmp/sys-metrics.json"),". The JSON syntax is as an array as follows:"),Object(s.b)("pre",null,Object(s.b)("code",Object(a.a)({parentName:"pre"},{className:"language-json"}),'[\n    {\n      "name": "accuracy", // Name of metric, should be string\n      "value": 0.98,      // Value of metric, should be a `number`\n      "format": ""        // Optional, valid values: "" or "%"\n    },\n    ...\n]\n')),Object(s.b)("p",null,"Here's an example in Python:"),Object(s.b)("pre",null,Object(s.b)("code",Object(a.a)({parentName:"pre"},{className:"language-python"}),"import json\n\n# JSON format for metrics\nmetrics = [\n    {'name': 'accuracy', 'value': 0.981},\n    {'name': 'loss', 'value': 0.018},\n]\n\n# Write metrics to `/tmp/sys-metrics.json`\nwith open('/tmp/sys-metrics.json', 'w') as f:\n    json.dump(metrics, f)\n")),Object(s.b)("p",null,"Once the Workflow task completes, you can view these metrics under ",Object(s.b)("strong",{parentName:"p"},"Artifacts")," in the task info panel:"),Object(s.b)("p",null,Object(s.b)("img",{src:t(233).default})),Object(s.b)("h2",{id:"passing-metrics-between-tasks"},"Passing metrics between tasks"),Object(s.b)("p",null,"Onepanel automatically outputs a ",Object(s.b)("inlineCode",{parentName:"p"},"sys-metrics")," artifact from a completed task, which you can access in a subsequent task as follows:"),Object(s.b)("pre",null,Object(s.b)("code",Object(a.a)({parentName:"pre"},{className:"language-yaml"}),"entrypoint: main\ntemplates:\n  - name: main\n    dag:\n      tasks:\n      - name: A\n        template: task-a-script\n      - name: B\n        dependencies: [A]\n        template: task-b-script\n        arguments:\n          # Use sys-metrics artifact output from task A\n          artifacts:\n          - name: task-a-metrics\n            from: \"{{tasks.A.outputs.artifacts.sys-metrics}}\"\n  - name: task-a-script\n    script:\n      image: python:3.7-alpine\n      command: [python, '-u']\n      source: |\n        import json\n        \n        # JSON format for metrics\n        metrics = [\n          {'name': 'accuracy', 'value': 0.981},\n          {'name': 'loss', 'value': 0.018},\n        ]\n        \n        # Write metrics to `/tmp/sys-metrics.json`\n        with open('/tmp/sys-metrics.json', 'w') as f:\n            json.dump(metrics, f)\n  \n  - name: task-b-script\n    inputs:\n      artifacts:\n      # Mount the argument to `/tmp/task-a-metrics.json` file\n      # You can optionally write to `/tmp/sys-metrics.json` to \n      # continously pass metrics to subsequent tasks.\n      - name: task-a-metrics\n        path: /tmp/task-a-metrics.json\n    script:\n      image: python:3.7-alpine\n      command: [python, '-u']\n      source: |\n        import json\n        \n        # Load Task A metrics\n        with open('/tmp/task-a-metrics.json') as f:\n            task_a_metrics = json.load(f)\n        \n        # Print to logs (optional)\n        print(task_a_metrics)\n")),Object(s.b)("h2",{id:"persisting-metrics-in-workflows"},"Persisting metrics in Workflows"),Object(s.b)("p",null,"You can use Onepanel's ",Object(s.b)("a",Object(a.a)({parentName:"p"},{href:"https://github.com/onepanelio/python-sdk#pip-install"}),"Python SDK")," to persist final metrics for a Workflow. These metrics are displayed in Workflow listing and detail pages and can also be edited using the web UI."),Object(s.b)("p",null,"Here's an example Python script that installs the SDK and persists metrics:"),Object(s.b)("pre",null,Object(s.b)("code",Object(a.a)({parentName:"pre"},{className:"language-python",metastring:"{7-8,10-13,38-67}","{7-8,10-13,38-67}":!0}),"import os\nimport datetime\nimport subprocess\nimport sys\nimport tensorflow as tf\n\n# Install onepanel-sdk\nsubprocess.check_call([sys.executable, '-m', 'pip', 'install', 'onepanel-sdk==0.16.0b10'])\n\nimport onepanel.core.api\nfrom onepanel.core.api.models.metric import Metric\nfrom onepanel.core.api.rest import ApiException\nfrom onepanel.core.api.models import Parameter\n\nmnist = tf.keras.datasets.mnist\n\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\ndef create_model():\n  return tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(10, activation='softmax')\n  ])\n\nmodel = create_model()\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(x=x_train, \n          y=y_train, \n          epochs=10, \n          validation_data=(x_test, y_test))\n\n# Set metrics variable to be passed into Onepanel SDK\nmetrics = [\n    {'name': 'accuracy', 'value': history.history['accuracy'][-1]},\n    {'name': 'loss', 'value': history.history['loss'][-1]}\n]\n\n# Get mounted service account token to use as API Key\nwith open('/var/run/secrets/kubernetes.io/serviceaccount/token') as f:\n    token = f.read()\n\n# Configure API authorization\nconfiguration = onepanel.core.api.Configuration(\n    host = os.getenv('ONEPANEL_API_URL'),\n    api_key = {\n        'authorization': token\n    }\n)\nconfiguration.api_key_prefix['authorization'] = 'Bearer'\n\nwith onepanel.core.api.ApiClient(configuration) as api_client:\n    api_instance = onepanel.core.api.WorkflowServiceApi(api_client)\n    namespace = '{{workflow.namespace}}'\n    uid = '{{workflow.name}}'\n    body = onepanel.core.api.AddWorkflowExecutionsMetricsRequest()\n    body.metrics = metrics\n    try:\n        api_response = api_instance.add_workflow_execution_metrics(namespace, uid, body)\n        print('Metrics added.')\n    except ApiException as e:\n        print(\"Exception when calling WorkflowServiceApi->add_workflow_execution_metrics: %s\\n\" % e)\n")),Object(s.b)("p",null,"You can use the above example in a Workflow script template as follows:"),Object(s.b)("pre",null,Object(s.b)("code",Object(a.a)({parentName:"pre"},{className:"language-yaml"}),"- name: metrics-example\n  script: \n    image: tensorflow/tensorflow:2.3.0\n    command: [python, '-u']\n    source: |\n      import os\n      import datetime\n      import subprocess\n      import sys\n      import tensorflow as tf\n\n      # Install onepanel-sdk\n      subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'onepanel-sdk==0.16.0b10'])\n\n      import onepanel.core.api\n      from onepanel.core.api.models.metric import Metric\n      from onepanel.core.api.rest import ApiException\n      from onepanel.core.api.models import Parameter\n\n      mnist = tf.keras.datasets.mnist\n\n      (x_train, y_train),(x_test, y_test) = mnist.load_data()\n      x_train, x_test = x_train / 255.0, x_test / 255.0\n\n      def create_model():\n        return tf.keras.models.Sequential([\n          tf.keras.layers.Flatten(input_shape=(28, 28)),\n          tf.keras.layers.Dense(512, activation='relu'),\n          tf.keras.layers.Dropout(0.2),\n          tf.keras.layers.Dense(10, activation='softmax')\n        ])\n\n      model = create_model()\n      model.compile(optimizer='adam',\n                    loss='sparse_categorical_crossentropy',\n                    metrics=['accuracy'])\n\n      history = model.fit(x=x_train, \n                y=y_train, \n                epochs=10, \n                validation_data=(x_test, y_test))\n\n      # Set metrics variable to be passed into Onepanel SDK\n      metrics = [\n          {'name': 'accuracy', 'value': history.history['accuracy'][-1]},\n          {'name': 'loss', 'value': history.history['loss'][-1]}\n      ]\n\n      # Get mounted service account token to use as API Key\n      with open('/var/run/secrets/kubernetes.io/serviceaccount/token') as f:\n          token = f.read()\n\n      # Configure API authorization\n      configuration = onepanel.core.api.Configuration(\n          host = os.getenv('ONEPANEL_API_URL'),\n          api_key = {\n              'authorization': token\n          }\n      )\n      configuration.api_key_prefix['authorization'] = 'Bearer'\n\n      with onepanel.core.api.ApiClient(configuration) as api_client:\n          api_instance = onepanel.core.api.WorkflowServiceApi(api_client)\n          namespace = 'rush'\n          uid = 'sdk-metrics-hdl27'\n          body = onepanel.core.api.AddWorkflowExecutionsMetricsRequest()\n          body.metrics = metrics\n          try:\n              api_response = api_instance.add_workflow_execution_metrics(namespace, uid, body)\n              print('Metrics added.')\n          except ApiException as e:\n              print(\"Exception when calling WorkflowServiceApi->add_workflow_execution_metrics: %s\\n\" % e)\n")),Object(s.b)("h2",{id:"advance-example"},"Advance example"),Object(s.b)("p",null,"This example shows a Workflow that saves metrics (",Object(s.b)("inlineCode",{parentName:"p"},"accuracy")," and ",Object(s.b)("inlineCode",{parentName:"p"},"loss"),") in two different tasks ",Object(s.b)("strong",{parentName:"p"},"A")," and ",Object(s.b)("strong",{parentName:"p"},"B"),", and then compares the accuracies in a subsequent task ",Object(s.b)("strong",{parentName:"p"},"C"),", and finally sends a Slack notification of the metrics with the best accuracy."),Object(s.b)("pre",null,Object(s.b)("code",Object(a.a)({parentName:"pre"},{className:"language-yaml"}),"entrypoint: main\ntemplates:\n  - name: main\n    dag:\n      tasks:\n      - name: A\n        template: task-a-script\n      - name: B\n        template: task-b-script\n      - name: C\n        dependencies: [A, B]\n        template: task-c-script\n        arguments:\n          artifacts:\n          - name: task-a-metrics\n            from: \"{{tasks.A.outputs.artifacts.sys-metrics}}\"\n          - name: task-b-metrics\n            from: \"{{tasks.B.outputs.artifacts.sys-metrics}}\"\n      - name: notify-in-slack\n        dependencies: [C]\n        template: slack-notify-success\n        arguments:\n          artifacts:\n          - name: task-c-metrics\n            from: \"{{tasks.C.outputs.artifacts.sys-metrics}}\"\n  - name: task-a-script\n    script:\n      image: python:3.7-alpine\n      command: [python, '-u']\n      source: |\n        import json\n        \n        # Training code here\n        \n        # JSON format for metrics\n        metrics = [\n          {'name': 'accuracy', 'value': 0.981},\n          {'name': 'loss', 'value': 0.018},\n        ]\n        \n        # Write metrics to `/tmp/sys-metrics.json`\n        with open('/tmp/sys-metrics.json', 'w') as f:\n            json.dump(metrics, f)\n        \n        # Print to logs (optional)\n        print(metrics)\n  \n  - name: task-b-script\n    script:\n      image: python:3.7-alpine\n      command: [python, '-u']\n      source: |\n        import json\n        \n        # Training code here\n        \n        # JSON format for metrics\n        metrics = [\n          {'name': 'accuracy', 'value': 0.972},\n          {'name': 'loss', 'value': 0.027},\n        ]\n        \n        # Write metrics to `/tmp/sys-metrics.json`\n        with open('/tmp/sys-metrics.json', 'w') as f:\n            json.dump(metrics, f)\n            \n        # Print to logs (optional)\n        print(metrics)\n  \n  - name: task-c-script\n    inputs:\n      artifacts:\n      - name: task-a-metrics\n        path: /tmp/task-a-metrics.json\n      - name: task-b-metrics\n        path: /tmp/task-b-metrics.json\n    script:\n      image: python:3.7-alpine\n      command: [python, '-u']\n      source: |\n        import json\n        \n        # Load Task A metrics\n        with open('/tmp/task-a-metrics.json') as f:\n            task_a_metrics = json.load(f)\n        \n        # Load Task B metrics\n        with open('/tmp/task-b-metrics.json') as f:\n            task_b_metrics = json.load(f)\n        \n        # Pick the metrics with best accuracy\n        task_a_accuracy = [m['value'] for m in task_a_metrics if m['name'] == 'accuracy'][0]\n        task_b_accuracy = [m['value'] for m in task_b_metrics if m['name'] == 'accuracy'][0]\n\n        if task_a_accuracy > task_b_accuracy:\n          metrics = task_a_metrics\n        else:\n          metrics = task_b_metrics\n        \n        # Write to metrics to `/tmp/sys-metrics.json`\n        with open('/tmp/sys-metrics.json', 'w') as f:\n            json.dump(metrics, f)\n            \n        # Print to logs (optional)\n        print(metrics)\n  \n  - name: slack-notify-success\n    container:\n      image: technosophos/slack-notify\n      command: [sh,-c]\n      args: ['SLACK_USERNAME=Worker SLACK_TITLE=\"{{workflow.name}} metrics\" SLACK_MESSAGE=$(cat /tmp/task-c-metrics.json)} ./slack-notify']\n    inputs:\n      artifacts:\n      - name: task-c-metrics\n        path: /tmp/task-c-metrics.json\n")))}m.isMDXComponent=!0}}]);